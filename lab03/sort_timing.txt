Input      Initial    Number     AvgTime            AvgTime
Size       Order      of Runs    for sortIntList    for sort
10000      random     5          0.22               0.00
10000      sorted     5          0.00               0.00 
10000      reverse    5          0.00               0.00 
20000      random     5          1.254              0.00
20000      sorted     5          0.005              0.00
20000      reverse    5          0.005              0.00
50000      random     5          10.948             0.01
50000      sorted     5          0.012              0.005
50000      reverse    5          0.018              0.01
TODO: add rows for larger input sizes
100000      random    5          49.67              0.03
100000      sorted    5          0.03               0.0175
100000      reverse   5          0.03               0.02


Explanation of timing results:

As input size increased we see an exponential increase in average time across
all types of initial order (reverse, random, sorted). 
This is due to length of scanning the sorting program will need to do;
as size increases time spent will also increase (time complexity).

In size 10,000 - we see a contant AvgTime within the sorted and reverse
order system of 0.00 sec (this is instant due to the ease of running through
the program in a sorted format i.e. 1,2,3,4...or ...4,3,2,1 -> ease of location).
In the random order list we can see the increase in time to 0.22 sec - indicating
that more time was spent (user time) due to the randomness( i.e. 1,3,6,2...).

In size 20,000 - AvgTime for sort file remained constant at 0.00 sec (this indicates
the more advanced and faster proccessing speed). For the AvgTime of sortIntList random
increased with larger data sets - the larger input in a random order computes to slower
proccessing speed. 

In size 50,000 - this similar fashion is followed, in sort files the time was approx. 0.01 sec
the larger input size indicated an increase in time spent. Similarly, random oder prodced the larger
time output (10.948 sec). An observation made here is that random indicates a larger change in time difference
in comparison to the other orders. 

In size 100,000 - this huge size produced a much larger time in all areas of testing. For example. the random
order produced 49.67 sec (average) on the sortIntList and 0.03 on sort. The exponential increase is contributed to the 
increase in input size. 

The time complexity is as follows:
 - random: O(n)
 - sorted: O(1)
 - reverse: O(1)



